# PointEdgeSegNet: Lightweight 3D Point Cloud Segmentation Model

## Overview

PointEdgeSegNet is the lightweight 3D point cloud segmentation model based on EdgeConv layers and U-Net architecture. This project has purpose of developing the neural network model that can classify each point in large-scale 3D point cloud into semantic categories such as ceiling, floor, wall, furniture, and other objects which you can customize easily. 

The model combines the effectiveness of EdgeConv for capturing local geometric relationships with the encoder-decoder structure of U-Net for preserving spatial information across different scales. This approach enables accurate semantic segmentation of complex indoor scenes. This model supports large-scale point cloud training and segmentation inference. Any point cloud format that can be used for training, such as files generated by a matterport or lidar scanner in the <X,Y,Z,R,G,B> format, can be used for training.

<p align="center">
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img4.png" height="150"></img>
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img3.png" height="150"></img>
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img1.png" height="150"></img></br>
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img5.png" height="200"></img>
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img2.png" height="200"></img></br>
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img9.png" height="115"></img>
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img6.png" height="115"></img>
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img8.png" height="115"></img>
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/img7.png" height="115"></img></br>
</p>
<p align="center">Example. Input point cloud and segments in output results using PointEdgeSegNet model(Val acc 73.7% at Epoch 30, Train dataset = S3DIS v1.2 aligned (Area1))
</p>

## Version

- 0.1: 2025/9/21. draft version.
- 0.2: 2025/9/24. CLI args, bug fixed.
- 0.3: 2025/9/26. GPU safety mode was added.

## Features

- Large-scale point cloud training and segmentation
- Edge-based convolution layers for capturing local geometric relationships
- U-Net encoder-decoder architecture with skip connections
- Lightweight Model Architecture
- Block-based processing for memory efficiency. Here, the concept of a block refers to a unit of point cloud file that divides a large number of point clouds, such as 100 million, into units that enable model learning and prediction within VRAM.
- Support for S3DIS dataset preprocessing and training
- Inference with 3D visualization of Large-scale point cloud file
- Comprehensive logging and model checkpointing

<p align="center">
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/figure_1.png" height="400"></img></br>
</p>
Loss and Acc performance [log](https://github.com/mac999/point_edge_seg_net/tree/main/logs/20250926_131906). Train and Val dataset = S3DIS v1.2 aligned (Area1) 

## Next plan
has 
- Diagnoise and visualize model related to weight, bais and gradient of loss in epoch.
- Customization about classes and features
- Finetuning
- LiDAR point cloud train example etc

## Installation

### Prerequisites

- Python 3.8 or higher
- NVIDIA GPU with CUDA support (recommended)
- Minimum 8GB GPU memory

### Step 1: Install PyTorch and PyTorch Geometric

Install PyTorch with CUDA support first (replace with your CUDA version):

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

Install PyTorch Geometric dependencies:

```bash
pip install torch_geometric torch_scatter torch_sparse torch_cluster torch_spline_conv
```

### Step 2: Install Other Dependencies

```bash
pip install numpy scikit-learn open3d tqdm matplotlib
```

### Step 3: Clone and Setup

```bash
git clone <repository-url>
cd point_unet
```

## Dataset Preparation

### Stanford 3D Indoor Spaces Dataset (S3DIS)

1. Download the S3DIS dataset from [Stanford Vision Lab](https://cvgl.stanford.edu/resources.html) and [point cloud storage](https://sdss.redivis.com/datasets/9q3m-9w5pa1a2h/files)
2. Extract the dataset to `./s3dis_v1.2_aligned/` directory
3. Run data preprocessing:

```bash
python data_preparation.py
```

The preprocessing script will:
- Convert raw point cloud data to PyTorch Geometric format
- Calculate geometric features (normals, curvature)
- Split data into 8192-point blocks for efficient training
- Save processed data to `./processed_s3dis/`

### Supported Classes

The model supports 13 semantic classes with their corresponding visualization colors:

| Class ID | Class Name | RGB Color | Color Name | Hex Code |
|----------|------------|-----------|------------|----------|
| 0 | ceiling | (233, 229, 107) | Light Yellow | #E9E56B |
| 1 | floor | (95, 156, 196) | Light Blue | #5F9CC4 |
| 2 | wall | (179, 116, 81) | Brown | #B37451 |
| 3 | beam | (241, 149, 131) | Light Coral | #F19583 |
| 4 | column | (81, 163, 163) | Teal | #51A3A3 |
| 5 | window | (223, 160, 168) | Light Pink | #DFA0A8 |
| 6 | door | (142, 86, 114) | Dark Pink | #8E5672 |
| 7 | table | (153, 223, 138) | Light Green | #99DF8A |
| 8 | chair | (149, 149, 241) | Light Purple | #9595F1 |
| 9 | sofa | (107, 229, 233) | Cyan | #6BE5E9 |
| 10 | bookcase | (233, 107, 229) | Magenta | #E96BE5 |
| 11 | board | (107, 233, 107) | Bright Green | #6BE96B |
| 12 | clutter | (160, 160, 160) | Gray | #A0A0A0 |

### Color Scheme Design

The color scheme is designed for optimal visual distinction:
- **Structural elements** (ceiling, floor, wall): Natural tones (yellow, blue, brown)
- **Architectural features** (beam, column, window, door): Warm and cool contrasts
- **Furniture** (table, chair, sofa, bookcase): Vibrant colors for easy identification
- **Functional items** (board): Bright green for visibility
- **Miscellaneous** (clutter): Neutral gray

## Model Architecture

### PointEdgeSegNet Theory

PointEdgeSegNet is built on two key concepts:

<p align="center">
<img src="https://github.com/mac999/point_edge_seg_net/blob/main/imgs/model1.png" height="500"></img>
</p>

#### 1. EdgeConv Layers

EdgeConv (Edge Convolution) is designed specifically for point cloud processing:

- Constructs dynamic k-nearest neighbor graphs for each point
- Captures local geometric relationships through edge features
- Combines point features with relative position information
- Maintains permutation invariance while being sensitive to local structure

Mathematical formulation:
```
edge_feature = MLP([x_i, x_j - x_i])
output_i = max(edge_feature) for all j in neighbors(i)
```

#### 2. U-Net Architecture

The encoder-decoder structure with skip connections:

**Encoder Path:**
- Stage 1: Input features -> 64 channels
- Stage 2: 64 -> 128 channels (4x downsampling)
- Stage 3: 128 -> 256 channels (4x downsampling)  
- Stage 4: 256 -> 512 channels (4x downsampling)

**Decoder Path:**
- Upsampling with k-NN interpolation
- Skip connections preserve fine-grained details
- Progressive feature fusion at each scale

### Network Flow

1. **Input Processing**: Raw point coordinates + RGB colors + geometric features (9D)
2. **Hierarchical Encoding**: Multi-scale feature extraction with progressive downsampling
3. **Bottleneck**: Highest-level feature representation
4. **Hierarchical Decoding**: Feature upsampling with skip connections
5. **Classification Head**: Point-wise classification to 13 semantic classes

### Key Innovations

- **Dynamic Graph Construction**: Adapts to local point density variations
- **Multi-scale Feature Fusion**: Combines global context with local details
- **Geometric Feature Integration**: Leverages Open3D for robust normal/curvature computation
- **Memory Efficient Processing**: Block-based training and inference

## Run

### Data Preparation

First, prepare your dataset:

```bash
# Basic data preparation (uses default paths)
python data_preparation.py

# Custom data preparation with specific areas
python data_preparation.py --input_path ./s3dis_v1.2_aligned --output_path ./processed_s3dis --areas Area_1 Area_2 Area_3
```

### Training

Basic training with default settings:

```bash
# Train with default configuration
python train_model.py

# Train with custom parameters
python train_model.py --num_epochs 50 --batch_size 8 --learning_rate 0.0005

# Train with specific areas and paths
python train_model.py --train_areas Area_1 Area_2 Area_3 --test_area Area_4 --num_epochs 25

# Full custom training example
python train_model.py \
    --processed_data_path ./custom_processed \
    --block_data_path ./custom_blocks \
    --train_areas Area_1 Area_2 Area_3 Area_4 Area_6 \
    --test_area Area_5 \
    --num_epochs 30 \
    --batch_size 4 \
    --learning_rate 0.001 \
    --block_size 8192
```

### Inference

Basic inference with default model:

```bash
# Basic inference (uses default model and test file)
python inference.py

# Inference with custom model and input
python inference.py --model_weights ./logs/20250924_053221/best_model.pth --input_cloud ./sample/area_6_conferenceRoom_1.txt

# Inference without visualization (for batch processing)
python inference.py --no_visualization

# Full custom inference example  
python inference.py \
    --model_weights ./custom_logs/best_model.pth \
    --input_cloud ./data/large_scene.txt \
    --block_path ./block_s3dis \
    --no_vis
```

### Common Run Scenarios

**Scenario 1: Quick Test Run**
```bash
# Minimal training for testing (5 epochs)
python train_model.py --num_epochs 5 --batch_size 2

# Quick inference test
python inference.py --no_visualization
```

**Scenario 2: Production Training**
```bash
# Full training with optimal settings
python train_model.py --num_epochs 50 --batch_size 8 --learning_rate 0.0005
```

**Scenario 3: Batch Processing**
```bash
# Process multiple files without visualization
for file in ./test_scenes/*.txt; do
    python inference.py --input_cloud "$file" --no_vis
done
```

**Scenario 4: Memory-Constrained Training**
```bash
# Reduce memory usage with smaller batch size and block size
python train_model.py --batch_size 2 --block_size 4096
```

### Available Arguments

**train_model.py arguments:**
- `--processed_data_path`: Path to processed data directory
- `--block_data_path`: Path to block data storage
- `--train_areas`: Training areas (space-separated list)
- `--test_area`: Test area name
- `--num_epochs`: Number of training epochs
- `--batch_size`: Batch size for training
- `--learning_rate`: Learning rate for optimizer
- `--num_features`: Number of input features (default: 9)
- `--num_classes`: Number of output classes (default: 13)
- `--block_size`: Size of point cloud blocks (default: 8192)

**inference.py arguments:**
- `-m, --model_weights`: Path to trained model weights (.pth)
- `-i, --input_cloud`: Path to input point cloud file (.txt)
- `-b, --block_path`: Directory for temporary inference blocks
- `--no_visualization, --no_vis`: Skip 3D visualization

## Training

### Quick Start

```bash
python train_model.py
```

### Configuration

Edit the configuration section in `train_model.py`:

```python
TRAIN_AREAS = ['Area_1', 'Area_2', 'Area_3', 'Area_4', 'Area_6']
TEST_AREA = 'Area_5'
NUM_EPOCHS = 30
BATCH_SIZE = 4
LEARNING_RATE = 0.001
BLOCK_SIZE = 8192
```

### Training Features

- Automatic train/validation split
- Learning rate scheduling with warmup
- Gradient clipping for stability
- Comprehensive logging (CSV + JSON)
- Model checkpointing (best and latest)
- Real-time loss visualization

### Output Structure

```
logs_YYYYMMDD_HHMMSS/
├── training_log.csv
├── config.json
├── best_model.pth
├── latest_model.pth
└── loss_curves.png
```

## Inference

### Basic Usage

```bash
python inference.py
```

### Inference Process

1. **Block Creation**: Splits input point cloud into 8192-point blocks (you can customize it)
2. **Feature Calculation**: Computes geometric features using Open3D
3. **Model Inference**: Processes each block through trained model
4. **Result Merging**: Combines predictions from all blocks
5. **Visualization**: Displays results with Open3D viewer

### Configuration

Edit the paths in `inference.py`:

```python
MODEL_WEIGHTS_PATH = './logs_YYYYMMDD_HHMMSS/best_model.pth'
TEST_POINT_CLOUD_PATH = './s3dis_v1.2_aligned/Area_1/conferenceRoom_1/conferenceRoom_1.txt'
```

## File Structure

```
point_unet/
├── model.py              # PointEdgeSegNet architecture
├── train_model.py        # Training script
├── inference.py          # Inference script
├── data_preparation.py   # Data preprocessing
├── requirements.txt      # Dependencies
├── README.md            # This file
├── s3dis_v1.2_aligned/  # Raw S3DIS dataset
├── processed_s3dis/     # Processed data
├── block_s3dis/         # Training blocks
├── inference_blocks/    # Inference blocks
└── logs_*/              # Training logs and models
```

## Performance

### Training Performance

- Training Time: ~2-4 hours on RTX 3080
- Memory Usage: ~6-8GB GPU memory
- Batch Size: 4 blocks per batch

### Inference Performance

- Processing Speed: ~1000-2000 points/second
- Memory Efficiency: Processes large point clouds in blocks

## Troubleshooting

### Common Issues

1. **CUDA Out of Memory**: Reduce BATCH_SIZE in training configuration
2. **Missing Dependencies**: Install PyTorch Geometric following official instructions
3. **Data Loading Errors**: Ensure S3DIS dataset structure matches expected format

### GPU Memory Optimization

- Use smaller batch sizes (BATCH_SIZE = 2-4)
- Enable mixed precision training
- Clear GPU cache between experiments

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs and feature requests.

## Author

- **Name**: Taewook Kang
- **Email**: laputa9999@gmail.com

## License

This project is released under the MIT License. See LICENSE file for details.

## Acknowledgments

- Stanford Vision Lab for the S3DIS dataset
- PyTorch Geometric team for the excellent graph neural network library
- Open3D team for 3D geometry processing tools




















